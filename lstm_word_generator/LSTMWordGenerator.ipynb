{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "from pandas.io.json import json_normalize\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "\n",
    "import json\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import tensorflow.keras.utils as ku \n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "import string\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BASE_DATA_PATH = os.path.join('fumseck', 'app', 'model')\n",
    "\n",
    "MODEL_LOSS = 'categorical_crossentropy'\n",
    "MODEL_OPTIMIZER = 'adam'\n",
    "MODEL_DENSE_ACTIVATION = 'softmax'\n",
    "MODEL_EMBEDDING_SIZE = 10 \n",
    "MODEL_LSTM_OUTPUT_DIM = 100\n",
    "MODEL_DROPOUT_RATE = 0.2\n",
    "\n",
    "EPOCHS = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed_value = 0):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    tf.set_random_seed(seed_value)\n",
    "    \n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_json(directory, filename):\n",
    "    path = os.path.join(directory, filename)\n",
    "    return pd.read_json(path)['quizz']\n",
    "\n",
    "dataframes = [read_json(BASE_DATA_PATH, dataset) for dataset in os.listdir(BASE_DATA_PATH)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_anecdotes(dataframes):\n",
    "    anecdotes = []\n",
    "    for i in range(0, len(dataframes)):\n",
    "        df = dataframes[i]\n",
    "        for level in ['debutant', 'confirme', 'expert']:\n",
    "            for j in range(0, len(df[level])):\n",
    "                anecdotes.append(df[level][j]['anecdote'])\n",
    "    return anecdotes\n",
    "\n",
    "anecdotes = get_anecdotes(dataframes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(anecdote):\n",
    "    anecdote = re.sub(\"'|«|»\", \" \", anecdote)\n",
    "    translator = str.maketrans(\"\", \"\", string.punctuation)\n",
    "    anecdote = anecdote.translate(translator)\n",
    "    anecdote = re.sub(\" +\", \" \", anecdote)\n",
    "    return anecdote.lower()\n",
    "\n",
    "anecdotes = [preprocess(anecdote) for anecdote in anecdotes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate sequence of N-grams tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "\n",
    "def get_sequence_of_tokens(corpus):\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    total_words = len(tokenizer.word_index) + 1\n",
    "    return total_words\n",
    "\n",
    "total_words = get_sequence_of_tokens(anecdotes)\n",
    "print(f\"There are {total_words} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "token_list = [tokenizer.texts_to_sequences([anecdote])[0] for anecdote in anecdotes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "\n",
    "def get_n_grams_sequence(corpus):\n",
    "    \n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    total_words = len(tokenizer.word_index) + 1\n",
    "     \n",
    "    input_sequences = []\n",
    "    for line in corpus:\n",
    "        token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "        for i in range(1, len(token_list)):\n",
    "            n_gram_sequence = token_list[:i+1]\n",
    "            input_sequences.append(n_gram_sequence)\n",
    "    return input_sequences, total_words\n",
    "    \n",
    "inp_sequences, total_words = get_n_grams_sequence(anecdotes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Padding sequences and feature-target dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_padded_sequences(input_sequences):\n",
    "    max_sequence_len = max([len(x) for x in input_sequences])\n",
    "    input_sequences = np.array(pad_sequences(input_sequences, maxlen = max_sequence_len, padding = 'pre'))\n",
    "    \n",
    "    predictors, label = input_sequences[:, :-1], input_sequences[:, -1]\n",
    "    label = ku.to_categorical(label, num_classes = total_words)\n",
    "    return predictors, label, max_sequence_len\n",
    "\n",
    "predictors, labels, max_sequence_len = generate_padded_sequences(inp_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Datas have shape, features : {predictors.shape}, targets: {label.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split in train-validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(predictors, labels, test_size=0.2, shuffle= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(max_sequence_len, total_words):\n",
    "    input_len = max_sequence_len - 1\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Embedding(input_dim = total_words, \n",
    "                        output_dim = MODEL_EMBEDDING_SIZE, \n",
    "                        input_length = input_len))\n",
    "    model.add(LSTM(MODEL_LSTM_OUTPUT_DIM))\n",
    "    model.add(Dropout(MODEL_DROPOUT_RATE))\n",
    "    model.add(Dense(total_words, activation = MODEL_DENSE_ACTIVATION))\n",
    "\n",
    "    model.compile(loss = MODEL_LOSS, optimizer = MODEL_OPTIMIZER)\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = create_model(max_sequence_len, total_words)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_ckpt_cb = ModelCheckpoint('text_gen_model.h5', monitor = 'val_loss', mode = 'min', save_best_only = True)\n",
    "es_cb = EarlyStopping(monitor = 'val_loss', mode = 'min', verbose = 1, patience = 10)\n",
    "callbacks = [model_ckpt_cb, es_cb]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = model.fit(x_train, \n",
    "                 y_train,\n",
    "                 validation_data = (x_val, y_val),\n",
    "                 epochs = EPOCHS, \n",
    "                 verbose = 1, \n",
    "                 callbacks = callbacks, \n",
    "                 shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook \n",
    "\n",
    "def plot_training_hist(hist): \n",
    "    plt.plot(hist.history['loss'])  \n",
    "    plt.plot(hist.history['val_loss'])\n",
    "    plt.title('model loss')  \n",
    "    plt.ylabel('loss')  \n",
    "    plt.xlabel('epoch')  \n",
    "    plt.legend(['train', 'valid'])\n",
    "    \n",
    "plot_training_hist(hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_text(seed_text, model, max_sequence_len):\n",
    "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "    token_list = pad_sequences([token_list], maxlen = max_sequence_len - 1, padding = 'pre')\n",
    "    predicted = model.predict(token_list, verbose = 0)\n",
    "    predicted = predicted[0].argsort()[-3:]\n",
    "    print(predicted.shape)\n",
    "    output_words = []\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index in predicted:\n",
    "            output_words.append(word)\n",
    "    return output_words\n",
    "\n",
    "# generate_text(\"Felix ne prend pas d\", model, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save vocabulary as json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_vocab_as_json(filename, words):\n",
    "    with open(filename, 'w') as json_file:\n",
    "          json.dump(words, json_file)\n",
    "            \n",
    "save_vocab_as_json('anecedots_vocab.json', tokenizer.word_docs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
